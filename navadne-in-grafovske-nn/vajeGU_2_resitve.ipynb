{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globoko učenje, vaje 2 - rekurenčne nevronske mreže\n",
    "\n",
    "Spoznali bomo modeliranje časovnih vrst s pomočjo rekurenčnih nevronskih mrež. Za vajo bomo sami implementirali rekurečno celico, GRU celico ter LSTM celico. Na koncu bomo pogledali še, kako lahko uporabimo pytorch-eve implementacije teh celic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Priprava podatkov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delali bomo s časovno vrsto ene spremenljivke. Za začetek uporabimo funkcijo sinus v N točkah. Podatke razdelimo na učno in testno množico, pri časovnih vrstah pa je pomembno, da se vedno učimo na preteklosti in napovedujemo prihodnost - podatkov torej ne smemo premešati! \n",
    "\n",
    "Učni primer za rekurenčno mrežo bo zaporedje dolžine M. Napiši funkcjo generiraj_zaporedja, ki sprejme časovno vrsto ter parameter M in zgenerira N-M zaporedij dolžine M (torej prvo od indeksa 0 do indeksa M, drugo od 1 do M+1, itd.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,799,400)\n",
    "y = torch.sin(x*2*np.pi/40) \n",
    "\n",
    "test_size = 100\n",
    "train_data = y[:-test_size]\n",
    "test_data = y[-test_size:]\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data)-seq_length):\n",
    "        sequences.append((data[i:i+seq_length], data[i+seq_length:i+seq_length+1]))\n",
    "    return sequences\n",
    "\n",
    "seq_length = 20\n",
    "train_sequences = create_sequences(train_data, seq_length)\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RNN celica\n",
    "\n",
    "### 2.1 RNN celica\n",
    "Implementirajmo svojo rekurečno celico myRNNCell ter mrežo myRNN, ki jo uporablja. Celica bo računala skrito stanje po enačbi\n",
    "$$h_t = \\tanh{(W_{ih}x_t + U_{hh}h_{t-1} + b_{ih} + b_{hh})},$$\n",
    "kjer je $x_t$ vhod (v našem primeru skalar), $h_{t-1}$ skrito stanje iz prejšnje iteracije (vektor izbrane dolžine), $W_{ih}, W{hh}, b_{ih}, b_{hh}$ pa so parametri linearne plasti nevronske mreže. Formulacijo lahko malo poenostavimo tako, da na vsaki iteraciji združimo vektorja $x_t$ in $h_{t-1}$ (recimo **torch.cat**) ter uporabljamo eno samo mrežo za združen vektor. \n",
    "\n",
    "Koristno je napisati še funkcjo, ki postavi vrednosti skritega stanja na 0.\n",
    "\n",
    "Dopolni!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(myRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.hidden = torch.zeros(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        combined = torch.cat((x, self.hidden))\n",
    "        self.hidden = self.tanh(self.i2h(combined))\n",
    "        return self.hidden\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = torch.zeros(self.hidden_size)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Mreža z RNN celico\n",
    "\n",
    "Zdaj napišimo še nevronsko mrežo, ki uporablja našo novo celico. Poleg RNN celice mreži dodaj še linearno plast, ki na podlagi skritega stanja napove $x_{t+1}$. Forward funkcija mreže bo na vsakem koraku prejela zaporedje kot vhod, zato mora implementirati zanko, ki elemente zaporedja poda RNN celici eno po eno, vrača pa $x_{t+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(myRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn_cell = myRNNCell(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.rnn_cell.reset_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for t in range(x.shape[0]):\n",
    "            h = self.rnn_cell(x[t].view(-1))\n",
    "            output += [self.output_layer(h)]\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        return output[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Učna zanka\n",
    "\n",
    "Zdaj potrebujemo še učno zanko. Najprej ustvarimo model, funkcijo napake (tokrat kar kvadratna napaka - nn.MSELoss) ter optimizator (kot zadnjič, torch.optim.Adam). Spiši učno zanko, ki mora zdaj na vsakem epohu izvesti še zanko po podzaporedjih. Pred vsakim zaporedjem postavi skrito stanje na 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myRNN(1, 32, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for i, (seq, labels) in enumerate(train_sequences):\n",
    "        optimizer.zero_grad()\n",
    "        model.reset_hidden()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(f'i: {i:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Evaluacija\n",
    "\n",
    "Da model evaluiramo, moramo spet iterirati skozi vse sekvence ter shranjevati napovedi. Ker napovedujemo numerično spremenljivko, uporabimo $R^2$ metriko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = train_data[-seq_length:].tolist()\n",
    "model = model.eval()\n",
    "for i in range(test_size):\n",
    "    seq = torch.FloatTensor(test_inputs[-seq_length:])\n",
    "    with torch.no_grad():\n",
    "        model.reset_hidden()\n",
    "        res = model(seq)\n",
    "        test_inputs.append(res.item())\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(test_data.numpy(), test_inputs[seq_length:]))\n",
    "\n",
    "actual_predictions = test_inputs[seq_length:]\n",
    "plt.plot(y.data.numpy()[-test_size:])\n",
    "plt.plot(actual_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Hiperparametri\n",
    "\n",
    "Rezultati verjetno niso najboljši, saj je delovanje rekurenčne mreže močno odvisno od hiperparametrov. Poskusi najti konfiguracijo, ki deluje dobro. Najpomembnejši parametri za variirati so dolžina sekvenc, število skritih stanj, learning rate ter število epohov."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GRU celica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 GRU celica\n",
    "\n",
    "je nagradnja RNN celice, ki se dinamično odloča, kaj naj pozabi in kaj naj si zapomni. Implementiraj GRU celico! Za osnovo lahko vzameš RNN celico iz 2.1 ter si pomagaš s prosojnicami s predavanj. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(myGRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.reset_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.update_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "        self.hidden = torch.zeros(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        combined = torch.cat((x, self.hidden))\n",
    "\n",
    "        rt = self.sig(self.reset_gate(combined))\n",
    "        combined2 = torch.cat((x, rt*self.hidden))\n",
    "        ht_hat = self.tanh(self.i2h(combined2))\n",
    "        zt = self.sig(self.update_gate(combined))\n",
    "        ht = zt*ht_hat + (1-zt)*self.hidden\n",
    "\n",
    "        self.hidden = ht.detach()\n",
    "\n",
    "        return self.hidden\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = torch.zeros(self.hidden_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Mreža z GRU celico\n",
    "\n",
    "Podobno kot v 2.2, pripravi mrežo, ki uporablja GRU celico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGRUnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(myGRUnet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru_cell = myGRUCell(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.gru_cell.reset_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for t in range(x.shape[0]):\n",
    "            h = self.gru_cell(x[t].view(-1))\n",
    "            output += [self.output_layer(h)]\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        return output[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Učna zanka, evaluacija, hiperparametri\n",
    "\n",
    "Spiši učno zanko, podobno kot v 2.3, ter jo evaluiraj. Najdi konfiguracijo hiperparametrov, ki deluje dobro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myGRUnet(1, 32, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for i, (seq, labels) in enumerate(train_sequences):\n",
    "        optimizer.zero_grad()\n",
    "        model.reset_hidden()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print(f'i: {i:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = train_data[-seq_length:].tolist()\n",
    "model = model.eval()\n",
    "for i in range(test_size):\n",
    "    seq = torch.FloatTensor(test_inputs[-seq_length:])\n",
    "    with torch.no_grad():\n",
    "        model.reset_hidden()\n",
    "        res = model(seq)\n",
    "        test_inputs.append(res.item())\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(test_data.numpy(), test_inputs[seq_length:]))\n",
    "\n",
    "actual_predictions = test_inputs[seq_length:]\n",
    "plt.plot(y.data.numpy()[-test_size:])\n",
    "plt.plot(actual_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM celica\n",
    "\n",
    "### 4.1 LSTM celica\n",
    "\n",
    "izboljša delovanje rekurenčne mreže za naloge, ko je potrebna obravnava daljše zgodovine. To doseže tako z dodatnim skritim stanjem, ki ima vlogo dolgoročnega spomina. Na podlagi GRU celice ter prosojnic s predavanj implementiraj LSTM celico!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(myLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.exit_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "        self.hidden = torch.zeros(hidden_size)\n",
    "        self.cell = torch.zeros(hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        combined = torch.cat((x, self.hidden))\n",
    "\n",
    "        ft = self.sig(self.forget_gate(combined))\n",
    "        it = self.sig(self.input_gate(combined))\n",
    "        ct_hat = self.tanh(self.i2h(combined))\n",
    "        ct = ft*self.cell + it*ct_hat\n",
    "        ot = self.sig(self.exit_gate(combined))\n",
    "        ht = ot * self.tanh(ct)\n",
    "\n",
    "        self.cell = ct.detach()\n",
    "        self.hidden = ht.detach()\n",
    "\n",
    "        return ht\n",
    "    \n",
    "    def reset_hidden(self):\n",
    "        self.hidden = torch.zeros(self.hidden_size)\n",
    "        self.cell = torch.zeros(self.hidden_size)\n",
    "\n",
    "class myLSTMnet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(myLSTMnet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru_cell = myLSTMCell(input_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def reset_hidden(self):\n",
    "        self.gru_cell.reset_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        for t in range(x.shape[0]):\n",
    "            h = self.gru_cell(x[t].view(-1))\n",
    "            output += [self.output_layer(h)]\n",
    "\n",
    "        output = torch.stack(output)\n",
    "        return output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myLSTMnet(1, 32, 1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch \", epoch)\n",
    "    for i, (seq, labels) in enumerate(train_sequences):\n",
    "        optimizer.zero_grad()\n",
    "        model.reset_hidden()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%100 == 0:\n",
    "            print(f'i: {i:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = train_data[-seq_length:].tolist()\n",
    "model = model.eval()\n",
    "for i in range(test_size):\n",
    "    seq = torch.FloatTensor(test_inputs[-seq_length:])\n",
    "    with torch.no_grad():\n",
    "        model.reset_hidden()\n",
    "        res = model(seq)\n",
    "        test_inputs.append(res.item())\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(r2_score(test_data.numpy(), test_inputs[seq_length:]))\n",
    "\n",
    "actual_predictions = test_inputs[seq_length:]\n",
    "plt.plot(y.data.numpy()[-test_size:])\n",
    "plt.plot(actual_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Težji podatki\n",
    "\n",
    "Preizkusi rekurenčne nevronske mreže še na težjih podatkih z dolgoročno odvisnostjo. Recimo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,799,400)\n",
    "y = 0.5*torch.sin(x*2*np.pi/40) + torch.sin(x*2*np.pi/400)\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boljše implementacije\n",
    "\n",
    "Naša \"domača\" implementacija sicer približno deluje, a manjka ji veliko podrobnosti: možnosti več plasti, učenje v \"batchih\", dropout plasti, itd. Preizkusi še katero od pytorchevih implementacij: torch.nn.RNN, torch.nn.GRU, torch.nn.LSTM. Za primer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x.view(len(x), 1, -1))\n",
    "        x = self.fc(x.view(len(x), -1))\n",
    "        return x[-1]\n",
    "\n",
    "\n",
    "\n",
    "model = RNN(1, 32, 1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (seq, labels) in enumerate(train_sequences):\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size))\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%100 == 0:\n",
    "            print(f'i: {i:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "test_inputs = train_data[-seq_length:].tolist()\n",
    "model = model.eval()\n",
    "\n",
    "for i in range(test_size):\n",
    "    seq = torch.FloatTensor(test_inputs[-seq_length:])\n",
    "    with torch.no_grad():\n",
    "        model.hidden = (torch.zeros(1,1,model.hidden_size))\n",
    "        test_inputs.append(model(seq).item())\n",
    "\n",
    "actual_predictions = test_inputs[seq_length:]\n",
    "plt.plot(y.data.numpy()[-test_size:])\n",
    "plt.plot(actual_predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
