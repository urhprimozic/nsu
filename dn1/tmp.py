slovar = {
    "ch": "Compute the Calinski and Harabasz index.",
    "int": "Compute the INT index.",
    "nre": "Compute the normalized relative entropy.",
    "pb": "Compute the pearson correlation between class matching and instance distances.",
    "sc": "Compute the number of clusters with size smaller than a given size.",
    "sil": "Compute the mean silhouette value.",
    "vdb": "Compute the Davies and Bouldin Index.",
    "vdu": "Compute the Dunn Index.",
    "c1": "Compute the entropy of class proportions.",
    "c2": "Compute the imbalance ratio.",
    "cls_coef": "Clustering coefficient.",
    "density": "Average density of the network.",
    "f1": "Maximum Fisher’s discriminant ratio.",
    "f1v": "Directional-vector maximum Fisher’s discriminant ratio.",
    "f2": "Volume of the overlapping region.",
    "f3": "Compute feature maximum individual efficiency.",
    "f4": "Compute the collective feature efficiency.",
    "hubs": "Hub score.",
    "l1": "Sum of error distance by linear programming.",
    "l2": "Compute the OVO subsets error rate of linear classifier.",
    "l3": "Non-Linearity of a linear classifier.",
    "lsc": "Local set average cardinality.",
    "n1": "Compute the fraction of borderline points.",
    "n2": "Ratio of intra and extra class nearest neighbor distance.",
    "n3": "Error rate of the nearest neighbor classifier.",
    "n4": "Compute the non-linearity of the k-NN Classifier.",
    "t1": "Fraction of hyperspheres covering data.",
    "t2": "Compute the average number of features per dimension.",
    "t3": "Compute the average number of PCA dimensions per points.",
    "t4": "Compute the ratio of the PCA dimension to the original dimension.",
    "cohesiveness": "Compute the improved version of the weighted distance, that captures how dense or sparse is the example distribution.",
    "conceptvar": "Compute the concept variation that estimates the variability of class labels among examples.",
    "impconceptvar": "Compute the improved concept variation that estimates the variability of class labels among examples.",
    "wg_dist": "Compute the weighted distance, that captures how dense or sparse is the example distribution.",
    "attr_to_inst": "Compute the ratio between the number of attributes.",
    "cat_to_num": "Compute the ratio between the number of categoric and numeric features.",
    "freq_class": "Compute the relative frequency of each distinct class.",
    "inst_to_attr": "Compute the ratio between the number of instances and attributes.",
    "nr_attr": "Compute the total number of attributes.",
    "nr_bin": "Compute the number of binary attributes.",
    "nr_cat": "Compute the number of categorical attributes.",
    "nr_class": "Compute the number of distinct classes.",
    "nr_inst": "Compute the number of instances (rows) in the dataset.",
    "nr_num": "Compute the number of numeric features.",
    "num_to_cat": "Compute the number of numerical and categorical features.",
    "attr_conc": "Compute concentration coef. of each pair of distinct attributes.",
    "attr_ent": "Compute Shannon’s entropy for each predictive attribute.",
    "class_conc": "Compute concentration coefficient between each attribute and class.",
    "class_ent": "Compute target attribute Shannon’s entropy.",
    "eq_num_attr": "Compute the number of attributes equivalent for a predictive task.",
    "joint_ent": "Compute the joint entropy between each attribute and class.",
    "mut_inf": "Compute the mutual information between each attribute and target.",
    "ns_ratio": "Compute the noisiness of attributes.",
    "one_itemset": "Compute the one itemset meta-feature.",
    "two_itemset": "Compute the two itemset meta-feature.",
    "best_node": "Performance of a the best single decision tree node.",
    "elite_nn": "Performance of Elite Nearest Neighbor.",
    "linear_discr": "Performance of the Linear Discriminant classifier.",
    "naive_bayes": "Performance of the Naive Bayes classifier.",
    "one_nn": "Performance of the 1-Nearest Neighbor classifier.",
    "random_node": "Performance of the single decision tree node model induced by a random attribute.",
    "worst_node": "Performance of the single decision tree node model induced by the worst informative attribute.",
    "leaves": "Compute the number of leaf nodes in the DT model.",
    "leaves_branch": "Compute the size of branches in the DT model.",
    "leaves_corrob": "Compute the leaves corroboration of the DT model.",
    "leaves_homo": "Compute the DT model Homogeneity for every leaf node.",
    "leaves_per_class": "Compute the proportion of leaves per class in DT model.",
    "nodes": "Compute the number of non-leaf nodes in DT model.",
    "nodes_per_attr": "Compute the ratio of nodes per number of attributes in DT model.",
    "nodes_per_inst": "Compute the ratio of non-leaf nodes per number of instances in DT model.",
    "nodes_per_level": "Compute the ratio of number of nodes per tree level in DT model.",
    "nodes_repeated": "Compute the number of repeated nodes in DT model.",
    "tree_depth": "Compute the depth of every node in the DT model.",
    "tree_imbalance": "Compute the tree imbalance for each leaf node.",
    "tree_shape": "Compute the tree shape for every leaf node.",
    "var_importance": "Compute the features importance of the DT model for each attribute.",
    "can_cor": "Compute canonical correlations of data.",
    "cor": "Compute the absolute value of the correlation of distinct dataset column pairs.",
    "cov": "Compute the absolute value of the covariance of distinct dataset attribute pairs.",
    "eigenvalues": "Compute the eigenvalues of covariance matrix from dataset.",
    "g_mean": "Compute the geometric mean of each attribute.",
    "gravity": "Compute the distance between minority and majority classes center of mass.",
    "h_mean": "Compute the harmonic mean of each attribute.",
    "iq_range": "Compute the interquartile range (IQR) of each attribute.",
    "kurtosis": "Compute the kurtosis of each attribute.",
    "lh_trace": "Compute the Lawley-Hotelling trace.",
    "mad": "Compute the Median Absolute Deviation (MAD) adjusted by a factor.",
    "max": "Compute the maximum value from each attribute.",
    "mean": "Compute the mean value of each attribute.",
    "median": "Compute the median value from each attribute.",
    "min": "Compute the minimum value from each attribute.",
    "nr_cor_attr": "Compute the number of distinct highly correlated pair of attributes.",
    "nr_disc": "Compute the number of canonical correlation between each attribute and class.",
    "nr_norm": "Compute the number of attributes normally distributed based in a given method.",
    "nr_outliers": "Compute the number of attributes with at least one outlier value.",
    "p_trace": "Compute the Pillai’s trace.",
    "range": "Compute the range (max - min) of each attribute.",
    "roy_root": "Compute the Roy’s largest root.",
    "sd": "Compute the standard deviation of each attribute.",
    "sd_ratio": "Compute a statistical test for homogeneity of covariances.",
    "skewness": "Compute the skewness for each attribute.",
    "sparsity": "Compute (possibly normalized) sparsity metric for each attribute.",
    "t_mean": "Compute the trimmed mean of each attribute.",
    "var": "Compute the variance of each attribute.",
    "w_lambda": "Compute the Wilks’ Lambda value.",
}

keys = [
    "attr_conc.mean",
    "attr_conc.sd",
    "attr_ent.mean",
    "attr_ent.sd",
    "attr_to_inst",
    "cat_to_num",
    "class_conc.mean",
    "class_conc.sd",
    "class_ent",
    "eq_num_attr",
    "freq_class.mean",
    "freq_class.sd",
    "joint_ent.mean",
    "joint_ent.sd",
    "mut_inf.mean",
    "mut_inf.sd",
    "nr_attr",
    "nr_cor_attr",
    "nr_norm",
    "nr_outliers",
    "ns_ratio",
    "sparsity.mean",
    "sparsity.sd",
]

for key in keys:
    tru_key = key.split(".")[0]
    print(key, " & ", slovar[tru_key], " \\\\")
